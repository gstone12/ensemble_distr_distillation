{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10811,
     "status": "ok",
     "timestamp": 1558128143256,
     "user": {
      "displayName": "Stratis Markou",
      "photoUrl": "",
      "userId": "09754366312766083286"
     },
     "user_tz": -60
    },
    "id": "xAYPKSFrG8AF",
    "outputId": "cf3df00b-2fcc-48ba-bd75-af624f50aa20"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/jakob/doktor/projects/EnsembleUncertainty/code\")\n",
    "\"\"\"Learing \"logit\" distribution in regression example\"\"\"\n",
    "import logging\n",
    "import zipfile\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.sgd import SGD\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn.model_selection import KFold\n",
    "from src.dataloaders.uci import uci_base, wine, bost\n",
    "from src import metrics\n",
    "from src import utils\n",
    "from src.ensemble import simple_regressor\n",
    "from src import loss as custom_loss\n",
    "\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "EXPERIMENT_NAME = \"red_regression_logits\"\n",
    "\n",
    "# Settings\n",
    "class Args():\n",
    "    pass\n",
    "args = Args()\n",
    "args.seed = 1\n",
    "args.gpu = False\n",
    "args.log_dir = Path(\"./logs\")\n",
    "args.log_level = logging.INFO\n",
    "args.retrain = True\n",
    "\n",
    "args.num_ensemble_members=1\n",
    "args.num_epochs=1\n",
    "args.lr = 0.01\n",
    "\n",
    "# General constructs\n",
    "rmse = metrics.Metric(name=\"RMSE\", function=metrics.root_mean_squared_error)\n",
    "BATCH_SIZE = 32\n",
    "torch.cuda.device(0)\n",
    "torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "device = torch.device(\"cuda\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1S5kt0omQ-N"
   },
   "outputs": [],
   "source": [
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        \n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "\n",
    "        out.append(v)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oAYelw3B5G-K"
   },
   "outputs": [],
   "source": [
    "def get_loss_and_rmse(network, loss_function, x, y, num_samples, mean_shift=None, std_scale=None):\n",
    "    x, y = to_variable(var=(x, y), cuda=True)\n",
    "\n",
    "    means, stds = [], []\n",
    "    for i in range(num_samples):\n",
    "        logits = network.forward(x)\n",
    "        output = network.transform_logits(logits)\n",
    "        means.append(output[0])\n",
    "        stds.append(output[1])\n",
    "    means, stds = torch.cat(means, dim=1), torch.cat(stds, dim=1)\n",
    "    mean = means.mean(dim=-1)[:, None]\n",
    "    std = ((means.var(dim=-1) + stds.mean(dim=-1)**2)**0.5)[:, None]\n",
    "\n",
    "    if mean_shift is not None and std_scale is not None:\n",
    "        mean_shift = torch.tensor(mean_shift).float().cuda()\n",
    "        std_scale = torch.tensor(std_scale).float().cuda()\n",
    "        mean = mean * std_scale + mean_shift\n",
    "        y = y * std_scale + mean_shift\n",
    "        std *= std_scale\n",
    "\n",
    "    loss = loss_function((mean, std), y)\n",
    "\n",
    "    rmse = ((mean - y)**2).mean()**0.5\n",
    "\n",
    "    return loss.detach().cpu(), rmse.detach().cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QBBlvk1JylkX"
   },
   "source": [
    "# UCI datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0AmcpT5DDO2d"
   },
   "outputs": [],
   "source": [
    "def train_mc_dropout(data,\n",
    "                     drop_prob,\n",
    "                     n_splits,\n",
    "                     num_epochs,\n",
    "                     num_units,\n",
    "                     learn_rate,\n",
    "                     weight_decay,\n",
    "                     log_every,\n",
    "                     num_samples,\n",
    "                     batch_size):\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    in_dim = data.shape[1] - 1\n",
    "    train_logliks, test_logliks = [], []\n",
    "    train_rmses, test_rmses = [], []\n",
    "    my_mse = metrics.Metric(name=\"MSE\", function=metrics.mean_squared_error)\n",
    "    \n",
    "    for j, idx in enumerate(kf.split(data)):\n",
    "        print('FOLD %d:' % j)\n",
    "        train_index, test_index = idx\n",
    "\n",
    "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
    "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
    "\n",
    "        x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
    "        y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
    "\n",
    "        x_train = (x_train - x_means) / x_stds\n",
    "        y_train = (y_train - y_means) / y_stds\n",
    "\n",
    "        x_test = (x_test - x_means) / x_stds\n",
    "        y_test = (y_test - y_means) / y_stds\n",
    "                \n",
    "        dataloader = uci_base.uci_dataloader(x_train, y_train, batch_size)\n",
    "\n",
    "        network = simple_regressor.Model(layer_sizes=[in_dim, num_units, 2],\n",
    "                                         device=device,\n",
    "                                         variance_transform=utils.variance_linear_asymptote(),\n",
    "                                         loss_function=custom_loss.gaussian_neg_log_likelihood_1d)\n",
    "                            \n",
    "        network.optimizer = torch.optim.SGD(network.parameters(),\n",
    "                                    lr=learn_rate,\n",
    "                                    weight_decay=weight_decay)\n",
    "\n",
    "        losses = []\n",
    "        fit_loss_train = np.zeros(num_epochs)\n",
    "        for i in range(num_epochs):\n",
    "            if batch_size is not None:\n",
    "                #loss = train_epoch(net, dataloader)\n",
    "                loss = network._train_epoch(dataloader)\n",
    "                \n",
    "        mean_shift = None\n",
    "        std_scale = None\n",
    "        #mean_shift = y_means\n",
    "        #std_scale = y_stds\n",
    "        train_loss, train_rmse = get_loss_and_rmse(network,\n",
    "                                                   network.loss,\n",
    "                                                   x_train,\n",
    "                                                   y_train,\n",
    "                                                   num_samples=num_samples,\n",
    "                                                   mean_shift=mean_shift,\n",
    "                                                   std_scale=std_scale)\n",
    "        \n",
    "        test_loss, test_rmse = get_loss_and_rmse(network,\n",
    "                                                 network.loss,\n",
    "                                                 x_test,\n",
    "                                                 y_test,\n",
    "                                                 num_samples=num_samples,\n",
    "                                                 mean_shift=mean_shift,\n",
    "                                                 std_scale=std_scale)\n",
    "        #my_mse.update(net.network(x_test), y_test)\n",
    "        train_logliks.append((train_loss.cpu().data.numpy()/len(x_train) + np.log(y_stds)[0]))\n",
    "        test_logliks.append((test_loss.cpu().data.numpy()/len(x_test) + np.log(y_stds)[0]))\n",
    "\n",
    "        train_rmses.append(y_stds[0]*train_rmse.cpu().data.numpy())\n",
    "        test_rmses.append(y_stds[0]*test_rmse.cpu().data.numpy())\n",
    "        print('Train loss: %6.3f Test loss: %6.3f RMSE: %.3f' % \n",
    "              (loss/len(x_train), test_loss/len(x_test), test_rmse*y_stds[0]))\n",
    "\n",
    "\n",
    "    print('Train log. lik. = %6.3f +/- %6.3f' % (-np.array(train_logliks).mean(),\n",
    "                                                 np.array(train_logliks).var()**0.5))\n",
    "    print('Test  log. lik. = %6.3f +/- %6.3f' % (-np.array(test_logliks).mean(),\n",
    "                                                 np.array(test_logliks).var()**0.5))\n",
    "    print('Train RMSE      = %6.3f +/- %6.3f' % (np.array(train_rmses).mean(),\n",
    "                                                 np.array(train_rmses).var()**0.5))\n",
    "    print('Test  RMSE      = %6.3f +/- %6.3f' % (np.array(test_rmses).mean(),\n",
    "                                                 np.array(test_rmses).var()**0.5))\n",
    "    #print('Test  RMSE      = %6.3f +/- %6.3f' % (np.array(my_rmses).mean(), np.array(my_rmses).var()**0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRPICBiXCegI"
   },
   "source": [
    "# Red wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4542,
     "status": "ok",
     "timestamp": 1558130529014,
     "user": {
      "displayName": "Stratis Markou",
      "photoUrl": "",
      "userId": "09754366312766083286"
     },
     "user_tz": -60
    },
    "id": "KOqgIBXcCegJ",
    "outputId": "21e7e896-5a30-4692-ff6d-72182f4b83b4"
   },
   "outputs": [],
   "source": [
    "wine_data = wine.WineData(\"data/uci/wine/winequality-red.csv\").data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6240,
     "status": "ok",
     "timestamp": 1558130531258,
     "user": {
      "displayName": "Stratis Markou",
      "photoUrl": "",
      "userId": "09754366312766083286"
     },
     "user_tz": -60
    },
    "id": "h8gWjBbKCegM",
    "outputId": "c09f3902-9869-4e50-d377-16d38d253446"
   },
   "outputs": [],
   "source": [
    "train_mc_dropout(data=wine_data,\n",
    "                       drop_prob=0.0,\n",
    "                       num_epochs=40,\n",
    "                       n_splits=10,\n",
    "                       num_units=50,\n",
    "                       learn_rate=1e-4,\n",
    "                       weight_decay=0.0, #1e-1/len(data)**0.5,\n",
    "                       num_samples=2,\n",
    "                       log_every=50,\n",
    "                       batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CA4b-sP4eBJw"
   },
   "source": [
    "# Housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4105,
     "status": "ok",
     "timestamp": 1558130357708,
     "user": {
      "displayName": "Stratis Markou",
      "photoUrl": "",
      "userId": "09754366312766083286"
     },
     "user_tz": -60
    },
    "id": "OHCuHqooeBJy",
    "outputId": "3f074919-5888-44fb-f31f-8aebe18e000b"
   },
   "outputs": [],
   "source": [
    "bost_data = bost.BostonData(\"data/uci/bost/housing.data\").data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5232,
     "status": "ok",
     "timestamp": 1558130359147,
     "user": {
      "displayName": "Stratis Markou",
      "photoUrl": "",
      "userId": "09754366312766083286"
     },
     "user_tz": -60
    },
    "id": "UXsgiUziqh9w",
    "outputId": "0b125922-91ea-4c97-904b-c6c6197e2a80"
   },
   "outputs": [],
   "source": [
    "net = train_mc_dropout(data=bost_data,\n",
    "                       drop_prob=0.0,\n",
    "                       num_epochs=40,\n",
    "                       n_splits=10,\n",
    "                       num_units=50,\n",
    "                       learn_rate=1e-4,\n",
    "                       weight_decay=0.0, #1e-1/len(data)**0.5,\n",
    "                       num_samples=20,\n",
    "                       log_every=50,\n",
    "                       batch_size=1411)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mc_dropout_heteroscedastic.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
