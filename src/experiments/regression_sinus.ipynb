{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from copy import deepcopy\n",
    "import os\n",
    "os.chdir(\"/home/jakob/doktor/projects/EnsembleUncertainty/code\")\n",
    "\"\"\"Learing \"logit\" distribution in regression example\"\"\"\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from src.dataloaders import gaussian_sinus, one_dim_regression\n",
    "import src.utils as utils\n",
    "from src.distilled import logits_probability_distribution\n",
    "from src.ensemble import ensemble\n",
    "from src.ensemble import sep_regressor, simple_regressor\n",
    "import src.metrics as metrics\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "EXPERIMENT_NAME = \"regression_logits\"\n",
    "\n",
    "# Settings\n",
    "class Args():\n",
    "    pass\n",
    "args = Args()\n",
    "args.seed = 1\n",
    "args.gpu = False\n",
    "args.log_dir = Path(\"./logs\")\n",
    "args.log_level = logging.INFO\n",
    "args.retrain = True\n",
    "\n",
    "args.num_ensemble_members=10\n",
    "args.num_epochs=100\n",
    "args.lr = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots(distilled_model, data):\n",
    "    test_loader = torch.utils.data.DataLoader(data,\n",
    "                                              batch_size=16,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=0)\n",
    "\n",
    "    predictions = np.zeros((data.n_samples, distilled_model.output_size))\n",
    "    all_x = np.zeros((data.n_samples, 1))\n",
    "    all_y = np.zeros((data.n_samples, 1))\n",
    "\n",
    "    idx = 0\n",
    "    for batch in test_loader:\n",
    "        inputs, targets = batch\n",
    "\n",
    "        predictions[idx * test_loader.batch_size:(idx + 1) * test_loader.batch_size, :, :] = \\\n",
    "            distilled_model.predict(inputs, t=None).data.numpy()\n",
    "\n",
    "        all_x[idx * test_loader.batch_size:(idx + 1) *\n",
    "              test_loader.batch_size, :] = inputs\n",
    "        all_y[idx * test_loader.batch_size:(idx + 1) *\n",
    "              test_loader.batch_size, :] = targets\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    plt.scatter(np.squeeze(all_x), np.squeeze(all_y), label=\"Data\", marker=\".\")\n",
    "\n",
    "    plt.errorbar(np.squeeze(all_x),\n",
    "                 predictions[:, 0],\n",
    "                 np.sqrt(predictions[:, 1]),\n",
    "                 label=\"Distilled model predictions\",\n",
    "                 marker=\".\",\n",
    "                 ls=\"none\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = Path(\"{}_{}.log\".format(\n",
    "    EXPERIMENT_NAME,\n",
    "    datetime.now().strftime(\"%Y%m%d_%H%M%S\")))\n",
    "utils.setup_logger(log_path=Path.cwd() / args.log_dir / log_file,\n",
    "                   log_level=args.log_level)\n",
    "LOGGER.info(\"Args: {}\".format(args))\n",
    "device = utils.torch_settings(args.seed, args.gpu)\n",
    "LOGGER.info(\"Creating dataloader\")\n",
    "data = gaussian_sinus.GaussianSinus(\n",
    "    store_file=Path(\"none\"))\n",
    "\n",
    "input_size = 1\n",
    "layer_sizes = [1, 10, 10, 1]\n",
    "ensemble_output_size = layer_sizes[-1] * 2\n",
    "args.num_ensemble_members = 2\n",
    "args.num_epochs=25\n",
    "args.lr = 0.001\n",
    "args.log_level = logging.INFO\n",
    "train_loader = torch.utils.data.DataLoader(data,\n",
    "                                           batch_size=32,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=0)\n",
    "\n",
    "prob_ensemble = ensemble.Ensemble(ensemble_output_size)\n",
    "for _ in range(args.num_ensemble_members):\n",
    "    model = sep_regressor.SepRegressor(layer_sizes,\n",
    "                                       device=device,\n",
    "                                       learning_rate=args.lr)\n",
    "    model.switch_active_network(\"mu\")\n",
    "    prob_ensemble.add_member(model)\n",
    "squared_error_metric = metrics.Metric(name=\"Squared error\",\n",
    "                                      function=metrics.mean_squared_error)\n",
    "prob_ensemble.add_metrics([squared_error_metric])\n",
    "prob_ensemble.train(train_loader, args.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in prob_ensemble.members:\n",
    "    model.switch_active_network(\"sigma_sq\")\n",
    "prob_ensemble.train(train_loader, args.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = -5\n",
    "end = 5\n",
    "step = 0.1\n",
    "x_length = int((end - start) / step)\n",
    "x = torch.arange(start=start,\n",
    "                 end=end,\n",
    "                 step=step).reshape((x_length, 1)).float()\n",
    "output = prob_ensemble.predict(x)\n",
    "x = x.detach().numpy()[:,0]\n",
    "\n",
    "def plot_uncert(ax, data, x, mean_mu=None, ale=None, epi=None):\n",
    "    inputs = data[:, :-1]\n",
    "    targets = data[:, -1]\n",
    "    ax.scatter(inputs, targets)\n",
    "    lower_x_bound = np.array([-3, -3])\n",
    "    upper_x_bound = np.array([3, 3])\n",
    "    y_bound = np.array([-2, 2])\n",
    "    ax.plot(lower_x_bound, y_bound, \"b--\")\n",
    "    ax.plot(upper_x_bound, y_bound, \"b--\")\n",
    "    if mean_mu is not None:\n",
    "        ax.plot(x, mean_mu, \"r-\", label=\"$\\mu_{avg}(x)$\")\n",
    "    if ale is not None:\n",
    "        every_nth = 3\n",
    "        ax.errorbar(x, mean_mu,\n",
    "                    np.sqrt(ale),\n",
    "                    errorevery=every_nth,\n",
    "                    color=\"r\",\n",
    "                    label=\"$E_w[\\sigma_w^2(x)]$\")\n",
    "        #ax.plot(x, ale, \"g-\", label=\"$E_w[\\sigma_w^2(x)]$\")\n",
    "    if epi is not None:\n",
    "        ax.fill_between(x, mean_mu + np.sqrt(epi), mean_mu - np.sqrt(epi),\n",
    "                        facecolor = \"blue\", alpha=0.5, label=\"var$_w(\\mu_w(x))$\")\n",
    "        #ax.plot(x, np.sqrt(100*epi))\n",
    "    plt.legend(prop={'size': 40})\n",
    "    plt.show()\n",
    "    \n",
    "mu = output[:, :, 0]\n",
    "var = output[:, :, 1]\n",
    "ale, epi = metrics.uncertainty_separation_parametric(mu, var)\n",
    "mean_mu = torch.mean(mu, dim=1).detach().numpy()\n",
    "ale = ale.detach().numpy()\n",
    "epi = epi.detach().numpy()\n",
    "#plot_uncert(data.get_full_data(), x, mean_mu, 10*ale, epi, ax)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [30, 30]\n",
    "_, ax = plt.subplots()\n",
    "\n",
    "plot_uncert(ax, data.get_full_data(), x, mean_mu=mean_mu, ale=1*ale, epi=1*epi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_reg_with_pred(data, x, pred_mean, pred_var, ax):\n",
    "    inputs = data[:, :-1]\n",
    "    targets = data[:, -1]\n",
    "    ax.scatter(inputs, targets)\n",
    "    ax.plot(x, means, 'r-')\n",
    "    every_nth = 10\n",
    "    ax.errorbar(x, means,\n",
    "                np.sqrt(var),\n",
    "                errorevery=every_nth,\n",
    "                color=\"r\")\n",
    "    plt.show()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [30, 30]\n",
    "member = 1\n",
    "means = output.detach().numpy()[:, member, 0]\n",
    "var = output.detach().numpy()[:, member, 1]\n",
    "_, ax = plt.subplots()\n",
    "plot_reg_with_pred(data.get_full_data(), x, means, var, ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create distilled!\n",
    "hidden_size = 10\n",
    "layer_sizes = [input_size, hidden_size, hidden_size, distilled_output_size]\n",
    "distilled_output_size = ensemble_output_size * 2\n",
    "distilled_model = logits_probability_distribution.LogitsProbabilityDistribution(\n",
    "    layer_sizes=layer_sizes,\n",
    "    teacher=prob_ensemble,\n",
    "    device=device,\n",
    "    learning_rate=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrain!\n",
    "lower = -5\n",
    "upper = 5\n",
    "unlabelled_data = gaussian_sinus.GaussianSinus(\n",
    "    store_file=Path(\"None\"), train=False, range_=(lower, upper))\n",
    "unlabelled_loader = torch.utils.data.DataLoader(unlabelled_data,\n",
    "                                           batch_size=6,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=0)\n",
    "\n",
    "\n",
    "distilled_model.train(unlabelled_loader, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reg_with_pred(data, x, pred_mean, pred_var, ax):\n",
    "    inputs = data[:, :-1]\n",
    "    targets = data[:, -1]\n",
    "    ax.scatter(inputs, targets)\n",
    "    ax.plot(x, pred_mean, 'r-')\n",
    "    every_nth = 10\n",
    "    ax.errorbar(x, pred_mean,\n",
    "                np.sqrt(pred_var),\n",
    "                errorevery=every_nth,\n",
    "                color=\"r\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_uncert(ax, data, x, mean_mu=None, ale=None, epi=None):\n",
    "    inputs = data[:, :-1]\n",
    "    targets = data[:, -1]\n",
    "    ax.scatter(inputs, targets)\n",
    "    lower_x_bound = np.array([-3, -3])\n",
    "    upper_x_bound = np.array([3, 3])\n",
    "    y_bound = np.array([-2, 2])\n",
    "    ax.plot(lower_x_bound, y_bound, \"b--\")\n",
    "    ax.plot(upper_x_bound, y_bound, \"b--\")\n",
    "    if mean_mu is not None:\n",
    "        ax.plot(x, mean_mu, \"r-\", label=\"$\\mu_{avg}(x)$\")\n",
    "    if ale is not None:\n",
    "        every_nth = 3\n",
    "        ax.errorbar(x, mean_mu,\n",
    "                    np.sqrt(ale),\n",
    "                    errorevery=every_nth,\n",
    "                    color=\"r\",\n",
    "                    label=\"$E_w[\\sigma_w^2(x)]$\")\n",
    "    if epi is not None:\n",
    "        ax.fill_between(x, mean_mu + np.sqrt(epi), mean_mu - np.sqrt(epi),\n",
    "                        facecolor = \"blue\", alpha=0.5, label=\"var$_w(\\mu_w(x))$\")\n",
    "        #ax.plot(x, np.sqrt(100*epi))\n",
    "    plt.legend(prop={'size': 40})\n",
    "    plt.show()\n",
    "    \n",
    "def compare_ale(ax, x, ale_ens, ale_dist):\n",
    "    mu = np.sin(x)\n",
    "    sigma = 0.15 * 1 / (1 + np.exp(-x))\n",
    "    ax.plot(x, sigma, label=\"true\")\n",
    "    ax.plot(x, ale_ens, label=\"Ens\")\n",
    "    ax.plot(x, ale_dist, label=\"Dist\")\n",
    "    ax.legend(prop={'size': 40})\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "start = -5\n",
    "end = 5\n",
    "step = 0.25\n",
    "x_length = int((end - start) / step)\n",
    "x = torch.arange(start=start,\n",
    "                 end=end,\n",
    "                 step=step, requires_grad=False).reshape((x_length, 1)).float()\n",
    "\n",
    "ens_output = prob_ensemble.predict(x)\n",
    "mu_ens = ens_output[:, :, 0]\n",
    "var_ens = ens_output[:, :, 1]\n",
    "ale_ens, epi_ens = metrics.uncertainty_separation_parametric(mu_ens, var_ens)\n",
    "ale_ens = ale_ens.detach().numpy()\n",
    "epi_ens = epi_ens.detach().numpy()\n",
    "z_mean, z_var = distilled_model.forward(x);\n",
    "z_mean = z_mean.detach().numpy()\n",
    "z_var = z_var.detach().numpy()\n",
    "x = x.detach().numpy()[:,0]\n",
    "\n",
    "mu = z_mean[:, 0]\n",
    "ale_dist = np.log( 1 + np.exp(z_mean[:, 1]))\n",
    "epi_dist = z_var[:, 1]\n",
    "\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [30, 30]\n",
    "_, ax = plt.subplots()\n",
    "#plot_reg_with_pred(data.get_full_data(), x, mu, sigma_sq, ax)\n",
    "\n",
    "#compare_ale(ax, x, epi_ens, epi_dist)\n",
    "#plot_data = gaussian_sinus.GaussianSinus(\n",
    "#    store_file=Path(\"None\"), train=False, range_=(-3, 3))\n",
    "plot_uncert(ax, unlabelled_data.get_full_data(), x, mean_mu=mu, ale=1*ale_dist, epi=1*epi_dist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.loss import gaussian_neg_log_likelihood\n",
    "x_test = torch.tensor((3.0)).reshape((1,1))\n",
    "ens_output = prob_ensemble.get_logits(x_test)\n",
    "print(\"Ens\", ens_output.shape)\n",
    "print(\"Ens mean\", ens_output.mean(1))\n",
    "z_mean, z_var = distilled_model.forward(x_test)\n",
    "mu = z_mean[0, :].reshape((1,2))\n",
    "cov = z_var[0, :].reshape((1,2))\n",
    "print(\"mu\", mu)\n",
    "print(\"cov\", cov)\n",
    "with torch.no_grad():\n",
    "    loss = gaussian_neg_log_likelihood((mu, cov), ens_output)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = -4\n",
    "end = 4\n",
    "step = 0.01\n",
    "x_length = int((end - start) / step)\n",
    "x = torch.arange(start=start,\n",
    "                 end=end,\n",
    "                 step=step, requires_grad=False).reshape((x_length, 1)).float()\n",
    "ensemble_preds = distilled_model._generate_teacher_predictions(x)\n",
    "ensemble_preds.shape\n",
    "\n",
    "plt.plot(x, ensemble_preds[:, :, 0].detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cov_mat = torch.Size([1])\n",
    "cov_mat = torch.eye(torch.Size([1]))\n",
    "diff = torch.ones(torch.Size([2, 1]))\n",
    "\n",
    "\n",
    "torch.matmul(diff, diff.T)\n",
    "diff.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
