{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10811,
     "status": "ok",
     "timestamp": 1558128143256,
     "user_tz": -60
    },
    "id": "xAYPKSFrG8AF",
    "outputId": "cf3df00b-2fcc-48ba-bd75-af624f50aa20"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"/home/jakob/doktor/projects/EnsembleUncertainty/code\")\n",
    "\"\"\"Learing \"logit\" distribution in regression example\"\"\"\n",
    "import logging\n",
    "import zipfile\n",
    "from copy import copy, deepcopy\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import Optimizer\n",
    "from torch.optim.sgd import SGD\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "from sklearn.model_selection import KFold\n",
    "from src.dataloaders.uci import uci_base, wine, bost\n",
    "from src import metrics\n",
    "from src import utils\n",
    "from src.ensemble import simple_regressor, ensemble\n",
    "from src.distilled import logits_probability_distribution, norm_inv_wish\n",
    "from src import loss as custom_loss\n",
    "import matplotlib.pyplot as plt\n",
    "import tikzplotlib\n",
    "\n",
    "# Settings\n",
    "class Args():\n",
    "    pass\n",
    "args = Args()\n",
    "args.seed = 1\n",
    "args.gpu = True\n",
    "args.log_dir = Path(\"./logs\")\n",
    "args.log_level = logging.WARNING\n",
    "args.retrain = True\n",
    "args.num_ensemble_members=1\n",
    "args.num_epochs=1\n",
    "args.lr = 0.01\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "EXPERIMENT_NAME = \"uci_wine\"\n",
    "\n",
    "log_file = Path(\"{}_{}.log\".format(\n",
    "    EXPERIMENT_NAME,\n",
    "    datetime.now().strftime(\"%Y%m%d_%H%M%S\")))\n",
    "utils.setup_logger(log_path=Path.cwd() / args.log_dir / log_file,\n",
    "                   log_level=args.log_level)\n",
    "\n",
    "# General constructs\n",
    "train_metrics = list()\n",
    "test_metrics = list()\n",
    "\n",
    "rmse = metrics.Metric(name=\"RMSE\", function=metrics.root_mean_squared_error)\n",
    "train_metrics.append(deepcopy(rmse))\n",
    "test_metrics.append(rmse)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "torch.cuda.device(0)\n",
    "torch.cuda.get_device_name(torch.cuda.current_device())\n",
    "device = torch.device(\"cuda\")\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "tex_dir = Path(\"/home/jakob/doktor/projects/EnsembleUncertainty/paper/Paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_1S5kt0omQ-N"
   },
   "outputs": [],
   "source": [
    "def to_variable(var=(), cuda=True, volatile=False):\n",
    "    out = []\n",
    "    for v in var:\n",
    "        \n",
    "        if isinstance(v, np.ndarray):\n",
    "            v = torch.from_numpy(v).type(torch.FloatTensor)\n",
    "\n",
    "        if not v.is_cuda and cuda:\n",
    "            v = v.cuda()\n",
    "\n",
    "        if not isinstance(v, Variable):\n",
    "            v = Variable(v, volatile=volatile)\n",
    "\n",
    "        out.append(v)\n",
    "    return out\n",
    "\n",
    "def get_loss_and_rmse(network,\n",
    "                      loss_function,\n",
    "                      x, y,\n",
    "                      mean_shift=None, std_scale=None):\n",
    "    x, y = to_variable(var=(x, y), cuda=True)\n",
    "\n",
    "    logits = network.forward(x)\n",
    "    output = network.transform_logits(logits)\n",
    "    mean, std = output\n",
    "\n",
    "    if mean_shift is not None and std_scale is not None:\n",
    "        mean_shift = torch.tensor(mean_shift).float().cuda()\n",
    "        std_scale = torch.tensor(std_scale).float().cuda()\n",
    "        mean = mean * std_scale + mean_shift\n",
    "        y = y * std_scale + mean_shift\n",
    "        std *= std_scale\n",
    "\n",
    "    loss = loss_function((mean, std), y)\n",
    "\n",
    "    rmse = ((mean - y)**2).mean()**0.5\n",
    "\n",
    "    return loss.detach().cpu(), rmse.detach().cpu()\n",
    "\n",
    "def create_ensemble(num_ensemble_members,\n",
    "                    input_dim,\n",
    "                    num_hidden,\n",
    "                    lr,\n",
    "                    ensemble_output_size):\n",
    "    prob_ensemble = ensemble.Ensemble(ensemble_output_size)\n",
    "    for _ in range(num_ensemble_members):\n",
    "        network = simple_regressor.Model(layer_sizes=[input_dim,\n",
    "                                                      num_hidden,\n",
    "                                                      ensemble_output_size],\n",
    "                                 device=device,\n",
    "                                 variance_transform=utils.positive_linear_asymptote(1e-6),\n",
    "                                 loss_function=custom_loss.gaussian_neg_log_likelihood_1d)\n",
    "        network.optimizer = torch.optim.Adam(network.parameters(),\n",
    "                                    lr=lr)\n",
    "        prob_ensemble.add_member(network)\n",
    "    return prob_ensemble\n",
    "\n",
    "def mean_and_std_from_list(samples):\n",
    "    \"\"\"Calculate mean and std from np-array compatible list\"\"\"\n",
    "    array = np.array(samples)\n",
    "    return array.mean(), array.std()\n",
    "\n",
    "def mean_and_std_from_metric(metric, rescale=1):\n",
    "    \"\"\"Calculate mean and std from np-array compatible list\"\"\"\n",
    "    return metric.mean() * rescale, metric.std() * rescale\n",
    "\n",
    "def test_distilled(distilled, x, y_true, device, scale):\n",
    "    with torch.no_grad():\n",
    "        num_samples = len(y_true)\n",
    "        x = torch.tensor(x).float().to(device)\n",
    "        z_mean, z_var = distilled.forward(x);\n",
    "        mu_dist = z_mean[:, 0].reshape(y_true.shape)\n",
    "        ale_dist = torch.log(1 + torch.exp(z_mean[:, 1]))\n",
    "        epi_dist = z_var[:, 1]\n",
    "        tot_uncert = ale_dist + epi_dist\n",
    "        y_true = torch.tensor(y_true,\n",
    "                              device=device,\n",
    "                              dtype=torch.float).reshape((num_samples, 1, 1))\n",
    "\n",
    "        rmse, nll, ause = common_test(y_true=y_true,\n",
    "                                      mu=mu_dist,\n",
    "                                      sigma_sq=tot_uncert,\n",
    "                                      uncert=tot_uncert)\n",
    "        rmse *= scale\n",
    "        nll += np.log(scale)\n",
    "        \n",
    "    return rmse.item(), nll.item(), ause.item()\n",
    "\n",
    "    \n",
    "def test_ensemble(prob_ensemble, x, y_true, device, scale):\n",
    "    with torch.no_grad():\n",
    "        num_samples = len(y_true)\n",
    "        output = prob_ensemble.predict(torch.tensor(x, device=device, dtype=torch.float))\n",
    "        mu_ens, sigma_sq_ens = output[:, :, 0],  output[:, :, 1]\n",
    "        mean_mu, tot_uncert = utils.gaussian_mixture_moments(mu_ens, sigma_sq_ens)\n",
    "        mean_mu = mean_mu.reshape((num_samples, 1)).to(device)\n",
    "        tot_uncert = tot_uncert.reshape((num_samples, 1)).to(device)\n",
    "        y_true = torch.tensor(y_true,\n",
    "                              device=device,\n",
    "                              dtype=torch.float).reshape((num_samples, 1, 1))\n",
    "\n",
    "        rmse, nll, ause = common_test(y_true, mean_mu, tot_uncert, tot_uncert)\n",
    "        rmse *= scale\n",
    "        nll += np.log(scale)\n",
    "    \n",
    "    return rmse.item(), nll.item(), ause.item()\n",
    "\n",
    "def common_test(y_true, mu, sigma_sq, uncert, num_partitions=10):\n",
    "    rmse = metrics.root_mean_squared_error(predictions=mu,\n",
    "                                           targets=y_true)\n",
    "    nll = custom_loss.gaussian_neg_log_likelihood_1d((mu, sigma_sq),\n",
    "                                                    y_true)\n",
    "    num_samples = len(y_true)\n",
    "    ause = ause_mix = utils.ause(y_true=y_true.reshape((num_samples, 1)),\n",
    "           y_pred=mu,\n",
    "           uncert_meas=uncert,\n",
    "           num_partitions=num_partitions)\n",
    "    return rmse, nll, ause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QBBlvk1JylkX"
   },
   "source": [
    "# UCI datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0AmcpT5DDO2d"
   },
   "outputs": [],
   "source": [
    "def train_ensemble(data,\n",
    "                     num_ensemble_members,\n",
    "                     num_epochs,\n",
    "                     num_units,\n",
    "                     n_splits,\n",
    "                     learn_rate,\n",
    "                     weight_decay,\n",
    "                     train_metrics,\n",
    "                     test_metrics,\n",
    "                     batch_size):\n",
    "\n",
    "    ens_rmses = list()\n",
    "    ens_nlls = list()\n",
    "    ens_auses = list()\n",
    "    \n",
    "    dist_rmses = list()\n",
    "    dist_nlls = list()\n",
    "    dist_auses = list()\n",
    "    \n",
    "    kf = KFold(n_splits=n_splits)\n",
    "    in_dim = data.shape[1] - 1\n",
    "    train_logliks, test_logliks = [], []\n",
    "    train_rmses, test_rmses = [], []\n",
    "    \n",
    "    hidden_size = 50\n",
    "    distilled_output_size = 4\n",
    "    layer_sizes = [in_dim, hidden_size, hidden_size, distilled_output_size]\n",
    "\n",
    "    for j, idx in enumerate(kf.split(data)):\n",
    "        train_index, test_index = idx\n",
    "        print(\"Fold: {}\".format(j))\n",
    "        for metric in train_metrics:\n",
    "            metric.reset()        \n",
    "\n",
    "        for metric in test_metrics:\n",
    "            metric.reset()\n",
    "\n",
    "        prob_ensemble = create_ensemble(num_ensemble_members=num_ensemble_members,\n",
    "                                        input_dim=in_dim,\n",
    "                                        num_hidden=num_units,\n",
    "                                        lr=learn_rate,\n",
    "                                        ensemble_output_size=2)\n",
    "        prob_ensemble.add_metrics(train_metrics)\n",
    "        \n",
    "        distilled_model = logits_probability_distribution.LogitsProbabilityDistribution(\n",
    "            layer_sizes=layer_sizes,\n",
    "            teacher=prob_ensemble,\n",
    "            variance_transform=utils.positive_linear_asymptote(),\n",
    "            device=device,\n",
    "            learning_rate=args.lr)\n",
    "\n",
    "\n",
    "        #x_train, y_train, x_test, y_test = data.create_train_val_split(0.9)\n",
    "        \n",
    "        x_train, y_train = data[train_index, :in_dim], data[train_index, in_dim:]\n",
    "        x_test, y_test = data[test_index, :in_dim], data[test_index, in_dim:]\n",
    "\n",
    "        x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
    "        y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
    "\n",
    "        x_train = (x_train - x_means) / x_stds\n",
    "        y_train = (y_train - y_means) / y_stds\n",
    "\n",
    "        x_test = (x_test - x_means) / x_stds\n",
    "        y_test = (y_test - y_means) / y_stds\n",
    "\n",
    "        data_std = y_stds[0]\n",
    "        if batch_size is None:\n",
    "            batch_size = x_train.shape[0]\n",
    "            \n",
    "        trainloader = uci_base.uci_dataloader(x_train, y_train, batch_size)\n",
    "        unlabelled_loader = uci_base.uci_dataloader(x_train, y_train, 128)\n",
    "\n",
    "\n",
    "        train_loss = prob_ensemble.train(train_loader=trainloader,\n",
    "                            num_epochs=num_epochs)\n",
    "        try:\n",
    "            distilled_model.train(unlabelled_loader, 30)\n",
    "        except (ValueError, RuntimeError):\n",
    "            print(\"NaN\")\n",
    "            continue\n",
    "            \n",
    "\n",
    "        rmse_ens, nll_ens, ause_ens = test_ensemble(prob_ensemble=prob_ensemble,\n",
    "                                                    x=x_test,\n",
    "                                                    y_true=y_test,\n",
    "                                                    device=device,\n",
    "                                                    scale=data_std)\n",
    "        rmse_dist, nll_dist, ause_dist = test_distilled(distilled_model,\n",
    "                                                     x=x_test,\n",
    "                                                     y_true=y_test,\n",
    "                                                     device=device,\n",
    "                                                     scale=data_std)\n",
    "            \n",
    "\n",
    "        ens_rmses.append(rmse_ens)\n",
    "        ens_nlls.append(nll_ens)\n",
    "        ens_auses.append(ause_ens)\n",
    "        \n",
    "        dist_rmses.append(rmse_dist)\n",
    "        dist_nlls.append(nll_dist)\n",
    "        dist_auses.append(ause_dist)\n",
    "        print(\"rmse: {}\\t nll: {}, ause: {}\".format(rmse_ens, nll_ens, ause_ens))\n",
    "        print(\"rmse: {}\\t nll: {}, ause: {}\".format(rmse_dist, nll_dist, ause_dist))\n",
    "\n",
    "        \n",
    "    #print(\"Train RMSE\\t = {:.3f} +/- {:.3f}\".format(\n",
    "    #    *mean_and_std_from_metric(train_metrics[0], rescale=data_std)))\n",
    "    #print(\"Train NLL\\t = {:.3f} +/- {:.3f}\".format(*mean_and_std_from_list(train_nll)))\n",
    "    print(\"Test RMSE\\t = {:.3f} +/- {:.3f}\".format(*mean_and_std_from_list(ens_rmses)))\n",
    "    print(\"Test NLL\\t = {:.3f} +/- {:.3f}\".format(*mean_and_std_from_list(ens_nlls)))\n",
    "    print(\"Test AUSE\\t = {:.3f} +/- {:.3f}\".format(*mean_and_std_from_list(ens_auses)))\n",
    "    print(\"Test RMSE\\t = {:.3f} +/- {:.3f}\".format(*mean_and_std_from_list(dist_rmses)))\n",
    "    print(\"Test NLL\\t = {:.3f} +/- {:.3f}\".format(*mean_and_std_from_list(dist_nlls)))\n",
    "    print(\"Test AUSE\\t = {:.3f} +/- {:.3f}\".format(*mean_and_std_from_list(dist_auses)))\n",
    "    \n",
    "    return (ens_rmses, ens_nlls, ens_auses), (dist_rmses, dist_nlls, dist_auses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HRPICBiXCegI"
   },
   "source": [
    "# Red wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4542,
     "status": "ok",
     "timestamp": 1558130529014,
     "user": {
      "displayName": "Stratis Markou",
      "photoUrl": "",
      "userId": "09754366312766083286"
     },
     "user_tz": -60
    },
    "id": "KOqgIBXcCegJ",
    "outputId": "21e7e896-5a30-4692-ff6d-72182f4b83b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0\n",
      "2020-02-20 12:05:16,115 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:05:16,116 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:05:16,117 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:05:16,118 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:05:16,119 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:05:16,120 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:05:16,121 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:05:16,122 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:05:16,123 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:05:16,124 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:05:16,125 WARNING LogitsProbabilityDistribution - Must assign proper loss function to child.loss.\n",
      "rmse: 0.6281677484512329\t nll: 0.9838426113128662, ause: 0.5055973529815674\n",
      "rmse: 0.6243095993995667\t nll: 1.0308181047439575, ause: 0.5926167964935303\n",
      "Fold: 1\n",
      "2020-02-20 12:07:33,070 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:07:33,071 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:07:33,073 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:07:33,075 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:07:33,076 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:07:33,078 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:07:33,079 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:07:33,080 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:07:33,081 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:07:33,081 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:07:33,082 WARNING LogitsProbabilityDistribution - Must assign proper loss function to child.loss.\n",
      "rmse: 0.6704588532447815\t nll: 1.0068762302398682, ause: 0.4715617299079895\n",
      "rmse: 0.6816164255142212\t nll: 1.0870518684387207, ause: 0.542247474193573\n",
      "Fold: 2\n",
      "2020-02-20 12:09:52,814 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:09:52,815 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:09:52,816 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:09:52,817 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:09:52,818 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:09:52,819 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:09:52,820 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:09:52,821 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:09:52,822 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:09:52,823 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:09:52,824 WARNING LogitsProbabilityDistribution - Must assign proper loss function to child.loss.\n",
      "rmse: 0.659213662147522\t nll: 0.9785832166671753, ause: 0.530312716960907\n",
      "rmse: 0.6537373661994934\t nll: 1.0616862773895264, ause: 0.64022296667099\n",
      "Fold: 3\n",
      "2020-02-20 12:12:13,372 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:12:13,373 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:12:13,373 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:12:13,374 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:12:13,376 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:12:13,377 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:12:13,378 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:12:13,379 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:12:13,380 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:12:13,382 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:12:13,382 WARNING LogitsProbabilityDistribution - Must assign proper loss function to child.loss.\n",
      "rmse: 0.6639246940612793\t nll: 0.9953003525733948, ause: 0.4877293109893799\n",
      "rmse: 0.6485989093780518\t nll: 1.05147123336792, ause: 0.5204840898513794\n",
      "Fold: 4\n",
      "2020-02-20 12:14:34,351 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:14:34,352 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:14:34,354 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:14:34,355 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:14:34,356 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:14:34,358 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:14:34,359 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:14:34,361 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:14:34,362 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:14:34,363 WARNING Ensemble        - Is subclass check disabled\n",
      "2020-02-20 12:14:34,364 WARNING LogitsProbabilityDistribution - Must assign proper loss function to child.loss.\n",
      "NaN\n",
      "Test RMSE\t = 0.655 +/- 0.016\n",
      "Test NLL\t = 0.991 +/- 0.011\n",
      "Test AUSE\t = 0.499 +/- 0.022\n",
      "Test RMSE\t = 0.652 +/- 0.020\n",
      "Test NLL\t = 1.058 +/- 0.020\n",
      "Test AUSE\t = 0.574 +/- 0.046\n"
     ]
    }
   ],
   "source": [
    "wine_data = wine.WineData(\"data/uci/wine/winequality-red.csv\")\n",
    "result  = train_ensemble(data=wine_data.data,\n",
    "                       num_ensemble_members=10,\n",
    "                       num_epochs=40,\n",
    "                       num_units=50,\n",
    "                       n_splits=5,\n",
    "                       learn_rate=1e-1,\n",
    "                       weight_decay=0.0, #1e-1/len(data)**0.5,\n",
    "                       train_metrics=train_metrics,\n",
    "                       test_metrics=test_metrics,\n",
    "                       batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prob_ensemble' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-f5be42c48c73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mx_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mens_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob_ensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmu_ens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mens_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mvar_ens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mens_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prob_ensemble' is not defined"
     ]
    }
   ],
   "source": [
    "x_train, y_train, _, _ = wine_data.create_train_val_split(1)\n",
    "\n",
    "x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
    "y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
    "\n",
    "x_train = (x_train - x_means) / x_stds\n",
    "y_train = (y_train - y_means) / y_stds\n",
    "\n",
    "x_tensor = torch.tensor(x_train).float().to(device)\n",
    "ens_output = prob_ensemble.predict(x_tensor)\n",
    "mu_ens = ens_output[:, :, 0]\n",
    "var_ens = ens_output[:, :, 1]\n",
    "mean_mu_ens = torch.mean(mu_ens, dim=1).reshape(y_train.shape).cpu().detach().numpy()\n",
    "\n",
    "ale_ens, epi_ens = metrics.uncertainty_separation_parametric(mu_ens, var_ens)\n",
    "ale_ens = ale_ens.detach().numpy()\n",
    "epi_ens = epi_ens.detach().numpy()\n",
    "\n",
    "z_mean, z_var = distilled_model.forward(x_tensor);\n",
    "z_mean = z_mean.cpu().detach()\n",
    "z_var = z_var.cpu().detach().numpy()\n",
    "mu_dist = z_mean[:, 0].reshape(y_train.shape).numpy()\n",
    "ale_dist = torch.log(1 + torch.exp(z_mean[:, 1])).numpy()\n",
    "epi_dist = z_var[:, 1]\n",
    "\n",
    "dist_spread = z_var.sum(1)\n",
    "window_size = 20\n",
    "uncert_ens = ale_ens + epi_ens\n",
    "fig, (ax_ens, ax_dist) = plt.subplots(1, 2, sharey=True)\n",
    "\n",
    "\n",
    "uncert_dist = ale_dist + epi_dist\n",
    "\n",
    "num_partitions = 10\n",
    "\n",
    "utils.plot_sparsification_error(ax_ens,\n",
    "                 y_true=y_train,\n",
    "                 y_pred=mean_mu_ens,\n",
    "                 uncert_meas=uncert_ens,\n",
    "                 num_partitions=num_partitions,\n",
    "                 label=\"Ensemble\")\n",
    "\n",
    "utils.plot_sparsification_error(ax_dist,\n",
    "                 y_true=y_train,\n",
    "                 y_pred=mu_dist,\n",
    "                 uncert_meas=uncert_dist,\n",
    "                 num_partitions=num_partitions,\n",
    "                 label=\"Distilled\")\n",
    "\n",
    "ax_ens.set_ylabel(\"$SE$\")\n",
    "reg_dir = Path(\"Experiments/Regression/fig/\")\n",
    "tikzplotlib.save(tex_dir/reg_dir/\"uci/wine_sparse_err.tikz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 50\n",
    "distilled_output_size = 4\n",
    "layer_sizes = [wine_data.input_dim, hidden_size, hidden_size, distilled_output_size]\n",
    "distilled_model = logits_probability_distribution.LogitsProbabilityDistribution(\n",
    "    layer_sizes=layer_sizes,\n",
    "    teacher=prob_ensemble,\n",
    "    variance_transform=utils.positive_linear_asymptote(),\n",
    "    device=device,\n",
    "    learning_rate=args.lr)\n",
    "\n",
    "unlabelled_loader = uci_base.uci_dataloader(x_train, y_train, 128)\n",
    "test_loader = uci_base.uci_dataloader(x_test, y_test, len(y_test))\n",
    "\n",
    "distilled_model.train(unlabelled_loader, 30)\n",
    "\n",
    "x_train, y_train, x_test, y_test = wine_data.create_train_val_split(0.9)\n",
    "x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
    "y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
    "\n",
    "x_train = (x_train - x_means) / x_stds\n",
    "y_train = (y_train - y_means) / y_stds\n",
    "\n",
    "x_test = (x_test - x_means) / x_stds\n",
    "y_test = (y_test - y_means) / y_stds\n",
    "\n",
    "unlabelled_loader = uci_base.uci_dataloader(x_train, y_train, 128)\n",
    "test_loader = uci_base.uci_dataloader(x_test, y_test, len(y_test))\n",
    "\n",
    "distilled_model.train(unlabelled_loader, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lr = args.lr / 10\n",
    "distilled_model.learning_rate = new_lr\n",
    "distilled_model.train(unlabelled_loader, 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = wine_data.create_train_val_split(0.9)\n",
    "x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
    "y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
    "\n",
    "x_train = (x_train - x_means) / x_stds\n",
    "y_train = (y_train - y_means) / y_stds\n",
    "\n",
    "x_test = (x_test - x_means) / x_stds\n",
    "y_test = (y_test - y_means) / y_stds\n",
    "test_nlls = list()\n",
    "test_rmses = list()\n",
    "with torch.no_grad():\n",
    "    x_test_tensor = torch.tensor(x_test, device=device, dtype=torch.float)\n",
    "    y_test_tensor = torch.tensor(y_test,\n",
    "                                     device=device,\n",
    "                                     dtype=torch.float).reshape((len(y_test), 1, 1))\n",
    "    \n",
    "    mean_dist, var_dist = distilled_model.forward(x_test_tensor)\n",
    "    mu_dist = mean_dist[:, 0].unsqueeze(1)\n",
    "    sigma_sq_dist = distilled_model.variance_transform(mean_dist[:, 0].unsqueeze(1))\n",
    "    test_rmse = metrics.root_mean_squared_error(predictions=mu_dist,\n",
    "                                    targets=y_test_tensor) * data_std\n",
    "    test_nll = custom_loss.gaussian_neg_log_likelihood_1d((mu_dist, sigma_sq_dist),\n",
    "                                                    y_test_tensor) + np.log(data_std)\n",
    "\n",
    "#train_nll.append(train_loss.item())\n",
    "test_nlls.append(test_nll.item())\n",
    "test_rmses.append(test_rmse.item())\n",
    "\n",
    "print(test_rmse)\n",
    "print(test_nll)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "        output = prob_ensemble.predict(torch.tensor(x_test, device=device, dtype=torch.float))\n",
    "        mean_mu_ens = mean_mu_ens.reshape((len(y_test), 1)).to(device)\n",
    "        mean_sigma_sq_ens = mean_sigma_sq_ens.reshape((len(y_test), 1)).to(device)\n",
    "        y_test_tensor = torch.tensor(y_test,\n",
    "                                     device=device,\n",
    "                                     dtype=torch.float).reshape((len(y_test), 1, 1))\n",
    "        test_rmse = metrics.root_mean_squared_error(predictions=mean_mu_ens,\n",
    "                                        targets=y_test_tensor) * data_std\n",
    "        test_nll = custom_loss.gaussian_neg_log_likelihood_1d((mean_mu_ens, mean_sigma_sq_ens),\n",
    "                                                        y_test_tensor) + np.log(data_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CA4b-sP4eBJw"
   },
   "source": [
    "# Housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5232,
     "status": "ok",
     "timestamp": 1558130359147,
     "user": {
      "displayName": "Stratis Markou",
      "photoUrl": "",
      "userId": "09754366312766083286"
     },
     "user_tz": -60
    },
    "id": "UXsgiUziqh9w",
    "outputId": "0b125922-91ea-4c97-904b-c6c6197e2a80"
   },
   "outputs": [],
   "source": [
    "bost_data = bost.BostonData(\"data/uci/bost/housing.data\")\n",
    "train_mc_dropout(data=bost_data,\n",
    "                 num_epochs=40,\n",
    "                 num_units=50,\n",
    "                 learn_rate=1e-4,\n",
    "                 weight_decay=0.0, #1e-1/len(data)**0.5,\n",
    "                 train_metrics=train_metrics,\n",
    "                 test_metrics=test_metrics,\n",
    "                 batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in unlabelled_loader:\n",
    "    x, y = batch\n",
    "    x, y = x.float().to(device), y.float().to(device)\n",
    "    logits = distilled_model(x)\n",
    "    output = distilled_model.transform_logits(logits)\n",
    "    break\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 50\n",
    "distilled_output_size = 4\n",
    "layer_sizes = [wine_data.input_dim, hidden_size, hidden_size, distilled_output_size]\n",
    "niw_dist = niw_probability_distribution.Model(layer_sizes=layer_sizes,\n",
    "                                              teacher=prob_ensemble,\n",
    "                                              device=device,\n",
    "                                              learning_rate=args.lr)\n",
    "\n",
    "x_train, y_train, x_test, y_test = wine_data.create_train_val_split(0.9)\n",
    "x_means, x_stds = x_train.mean(axis = 0), x_train.var(axis = 0)**0.5\n",
    "y_means, y_stds = y_train.mean(axis = 0), y_train.var(axis = 0)**0.5\n",
    "\n",
    "x_train = (x_train - x_means) / x_stds\n",
    "y_train = (y_train - y_means) / y_stds\n",
    "\n",
    "x_test = (x_test - x_means) / x_stds\n",
    "y_test = (y_test - y_means) / y_stds\n",
    "\n",
    "unlabelled_loader = uci_base.uci_dataloader(x_train, y_train, 32)\n",
    "test_loader = uci_base.uci_dataloader(x_test, y_test, len(y_test))\n",
    "\n",
    "niw_dist.train(unlabelled_loader, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_dir = Path(\"Experiments/Regression/data/\")\n",
    "utils.csv_result(result, file=tex_dir/reg_dir/(EXPERIMENT_NAME + \".csv\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "mc_dropout_heteroscedastic.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
