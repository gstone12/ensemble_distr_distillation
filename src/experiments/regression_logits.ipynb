{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from copy import deepcopy\n",
    "import os\n",
    "os.chdir(\"/home/jakob/doktor/projects/EnsembleUncertainty/code\")\n",
    "\"\"\"Learing \"logit\" distribution in regression example\"\"\"\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from src.dataloaders import one_dim_regression\n",
    "import src.utils as utils\n",
    "from src.distilled import logits_probability_distribution\n",
    "from src.ensemble import ensemble\n",
    "from src.ensemble import simple_regressor\n",
    "import src.metrics as metrics\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "EXPERIMENT_NAME = \"regression_logits\"\n",
    "\n",
    "# Settings\n",
    "class Args():\n",
    "    pass\n",
    "args = Args()\n",
    "args.seed = 1\n",
    "args.gpu = False\n",
    "args.log_dir = Path(\"./logs\")\n",
    "args.log_level = logging.DEBUG\n",
    "args.retrain = True\n",
    "\n",
    "args.num_ensemble_members=10\n",
    "args.num_epochs=10\n",
    "args.lr = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots(distilled_model, data):\n",
    "    test_loader = torch.utils.data.DataLoader(data,\n",
    "                                              batch_size=16,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=0)\n",
    "\n",
    "    predictions = np.zeros((data.n_samples, distilled_model.output_size))\n",
    "    all_x = np.zeros((data.n_samples, 1))\n",
    "    all_y = np.zeros((data.n_samples, 1))\n",
    "\n",
    "    idx = 0\n",
    "    for batch in test_loader:\n",
    "        inputs, targets = batch\n",
    "\n",
    "        predictions[idx * test_loader.batch_size:(idx + 1) * test_loader.batch_size, :, :] = \\\n",
    "            distilled_model.predict(inputs, t=None).data.numpy()\n",
    "\n",
    "        all_x[idx * test_loader.batch_size:(idx + 1) *\n",
    "              test_loader.batch_size, :] = inputs\n",
    "        all_y[idx * test_loader.batch_size:(idx + 1) *\n",
    "              test_loader.batch_size, :] = targets\n",
    "\n",
    "        idx += 1\n",
    "\n",
    "    plt.scatter(np.squeeze(all_x), np.squeeze(all_y), label=\"Data\", marker=\".\")\n",
    "\n",
    "    plt.errorbar(np.squeeze(all_x),\n",
    "                 predictions[:, 0],\n",
    "                 np.sqrt(predictions[:, 1]),\n",
    "                 label=\"Distilled model predictions\",\n",
    "                 marker=\".\",\n",
    "                 ls=\"none\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = Path(\"{}_{}.log\".format(\n",
    "    EXPERIMENT_NAME,\n",
    "    datetime.now().strftime(\"%Y%m%d_%H%M%S\")))\n",
    "utils.setup_logger(log_path=Path.cwd() / args.log_dir / log_file,\n",
    "                   log_level=args.log_level)\n",
    "LOGGER.info(\"Args: {}\".format(args))\n",
    "device = utils.torch_settings(args.seed, args.gpu)\n",
    "LOGGER.info(\"Creating dataloader\")\n",
    "data = one_dim_regression.SyntheticRegressionData(\n",
    "    store_file=Path(\"data/2d_gaussian_1000\"))\n",
    "\n",
    "input_size = 1\n",
    "hidden_size = 5\n",
    "ensemble_output_size = 2\n",
    "train_loader = torch.utils.data.DataLoader(data,\n",
    "                                           batch_size=6,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=0)\n",
    "\n",
    "prob_ensemble = ensemble.Ensemble(ensemble_output_size)\n",
    "for _ in range(args.num_ensemble_members):\n",
    "    model = simple_regressor.SimpleRegressor(input_size,\n",
    "                                             hidden_size,\n",
    "                                             hidden_size,\n",
    "                                             ensemble_output_size,\n",
    "                                             device=device,\n",
    "                                             learning_rate=args.lr)\n",
    "    prob_ensemble.add_member(model)\n",
    "squared_error_metric = metrics.Metric(name=\"Squared error\",\n",
    "                                      function=metrics.squared_error)\n",
    "prob_ensemble.add_metrics([squared_error_metric])\n",
    "prob_ensemble.train(train_loader, args.num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = -5\n",
    "end = 6\n",
    "step = 0.1\n",
    "x_length = int((end - start) / step)\n",
    "x = torch.arange(start=start,\n",
    "                 end=end,\n",
    "                 step=step).reshape((x_length, 1)).float()\n",
    "output = prob_ensemble.predict(x)\n",
    "x = x.detach().numpy()[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_uncert(data, x, mean_mu, ale, epi, ax):\n",
    "    inputs = data[:, :-1]\n",
    "    targets = data[:, -1]\n",
    "    ax.scatter(inputs, targets)\n",
    "    ax.plot(x, mean_mu, \"r-\", label=\"$\\mu_{avg}(x)$\")\n",
    "    ax.plot(x, 0.2*ale, \"g-\", label=\"$E_w[\\sigma_w^2(x)]$\")\n",
    "    ax.plot(x, epi, \"b-\", label=\"var$_w(\\mu_w(x))$\")\n",
    "    plt.legend(prop={'size': 40})\n",
    "    plt.show()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [30, 30]\n",
    "\n",
    "mu = output[:, :, 0]\n",
    "var = output[:, :, 1]\n",
    "ale, epi = metrics.uncertainty_separation_parametric(mu, var)\n",
    "mean_mu = torch.mean(mu, dim=1).detach().numpy()\n",
    "ale = ale.detach().numpy()\n",
    "epi = epi.detach().numpy()\n",
    "_, ax = plt.subplots()\n",
    "plot_uncert(data.get_full_data(), x, mean_mu, ale, epi, ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_reg_with_pred(data, x, pred_mean, pred_var, ax):\n",
    "    inputs = data[:, :-1]\n",
    "    targets = data[:, -1]\n",
    "    ax.scatter(inputs, targets)\n",
    "    ax.plot(x, means, 'r-')\n",
    "    every_nth = 10\n",
    "    ax.errorbar(x, means,\n",
    "                np.sqrt(var),\n",
    "                errorevery=every_nth,\n",
    "                color=\"r\")\n",
    "    plt.show()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [30, 30]\n",
    "member = 9\n",
    "means = output.detach().numpy()[:, member, 0]\n",
    "var = output.detach().numpy()[:, member, 1]\n",
    "_, ax = plt.subplots()\n",
    "plot_reg_with_pred(data.get_full_data(), x, means, var, ax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabelled_data = one_dim_regression.SyntheticRegressionData(\n",
    "    store_file=Path(\"data/2d_gaussian_1000\"), train=False)\n",
    "unlabelled_loader = torch.utils.data.DataLoader(unlabelled_data,\n",
    "                                           batch_size=6,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=0)\n",
    "\n",
    "distilled_output_size = ensemble_output_size * 2\n",
    "distilled_model = logits_probability_distribution.LogitsProbabilityDistribution(\n",
    "    input_size,\n",
    "    hidden_size,\n",
    "    hidden_size,\n",
    "    distilled_output_size,\n",
    "    teacher=prob_ensemble,\n",
    "    device=device,\n",
    "    learning_rate=args.lr)\n",
    "distilled_model.train(unlabelled_loader, 100)\n",
    "\n",
    "#distilled_model._train_epoch(unlabelled_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = one_dim_regression.SyntheticRegressionData(\n",
    "    store_file=Path(\"data/2d_gaussian_1000\"))\n",
    "\n",
    "start = -6\n",
    "end = 4\n",
    "step = 0.1\n",
    "x_length = int((end - start) / step)\n",
    "x = torch.arange(start=start,\n",
    "                 end=end,\n",
    "                 step=step, requires_grad=False).reshape((x_length, 1)).float()\n",
    "\n",
    "z_mean, z_var = distilled_model.forward(x);\n",
    "x = x.detach().numpy()[:,0]\n",
    "\n",
    "mu = z_mean[:, 0].detach().numpy()\n",
    "sigma_sq = torch.exp(z_mean[:, 1]).detach().numpy()\n",
    "\n",
    "epi_var = z_var[:, 0].detach().numpy()\n",
    "\n",
    "def plot_reg_with_pred(data, x, pred_mean, pred_var, ax):\n",
    "    inputs = data[:, :-1]\n",
    "    targets = data[:, -1]\n",
    "    ax.scatter(inputs, targets)\n",
    "    ax.plot(x, pred_mean, 'r-')\n",
    "    every_nth = 10\n",
    "    ax.errorbar(x, pred_mean,\n",
    "                np.sqrt(pred_var),\n",
    "                errorevery=every_nth,\n",
    "                color=\"r\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_uncert(ax, data, x, mean_mu=None, ale=None, epi=None):\n",
    "    inputs = data[:, :-1]\n",
    "    targets = data[:, -1]\n",
    "    ax.scatter(inputs, targets)\n",
    "    if mean_mu is not None:\n",
    "        ax.plot(x, mean_mu, \"r-\", label=\"$\\mu_{avg}(x)$\")\n",
    "    if ale is not None:\n",
    "        ax.plot(x, ale, \"g-\", label=\"$E_w[\\sigma_w^2(x)]$\")\n",
    "    if epi is not None:\n",
    "        ax.plot(x, epi, \"b-\", label=\"var$_w(\\mu_w(x))$\")\n",
    "    plt.legend(prop={'size': 40})\n",
    "    plt.show()\n",
    "    \n",
    "plt.rcParams['figure.figsize'] = [30, 30]\n",
    "_, ax = plt.subplots()\n",
    "#plot_reg_with_pred(data.get_full_data(), x, mu, sigma_sq, ax)\n",
    "\n",
    "plot_uncert(ax, data.get_full_data(), x, mean_mu=mu, ale=0.1*sigma_sq, epi=epi_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = -4\n",
    "end = 4\n",
    "step = 0.01\n",
    "x_length = int((end - start) / step)\n",
    "x = torch.arange(start=start,\n",
    "                 end=end,\n",
    "                 step=step, requires_grad=False).reshape((x_length, 1)).float()\n",
    "ensemble_preds = distilled_model._generate_teacher_predictions(x)\n",
    "ensemble_preds.shape\n",
    "\n",
    "plt.plot(x, ensemble_preds[:, :, 0].detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled_model.train(unlabelled_loader, 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cov_mat = torch.Size([1])\n",
    "cov_mat = torch.eye(torch.Size([1]))\n",
    "diff = torch.ones(torch.Size([2, 1]))\n",
    "\n",
    "\n",
    "torch.matmul(diff, diff.T)\n",
    "diff.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
